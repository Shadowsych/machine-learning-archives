{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words (BoW)\n",
    "An NLP model that represents a text (such as a sentence or document) as a bag (multiset) of its words, disregarding grammar and even word order.\n",
    "\n",
    "### Example of BoW\n",
    "Sentence 1: ```John likes to watch movies. Mary likes movies too.```  \n",
    "Sentence 2: ```John also likes to watch football games.```\n",
    "\n",
    "Put both sentences into their individual Bags, in Python we use a dictionary to abstract it.\n",
    "```python\n",
    "BoW1 = {\"John\": 1, \"likes\": 2, \"to\": 1, \"watch\": 1, \"movies\": 2, \"Mary\": 1, \"too\": 1}\n",
    "BoW2 = {\"John\": 1,\"also\": 1,\"likes\": 1,\"to\": 1, \"watch\": 1,\"football\": 1,\"games\": 1}\n",
    "```\n",
    "The key represents the word, and the value is the frequency of the word.\n",
    "\n",
    "Now we need to \"normalize\" the item frequencies by removing common words such as prepositions like \"to\", \"as\", \"the\", and etc. from each Bag of words. Another normalization technique is to remove stems from the words such as \"loved\" becomes \"love\" or \"plays\" becomes \"play\" and etc.\n",
    "\n",
    "Then we can compare the words and their frequencies in each sentence and see if the two sentences are alike. This would work best on a large data set of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import the data set use tab delimeter because the file is a tsv\n",
    "- quoting = 3 ignore all double quotes\n",
    "\"\"\"\n",
    "ignore_quotes = 3\n",
    "reviews = pd.read_csv(\"datasets/restaurant_reviews.tsv\", delimiter=\"\\t\", quoting=ignore_quotes)\n",
    "\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing The Texts\n",
    "We need to clean the texts before fitting and predicting on the Bag of Words model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import regular expressions and NLTK\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/pravat/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the \"stop words\" list (a list of irrelevant words)\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# import the porter stemmer, an algorithm to strip suffixes and receive only the word stem\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a cleaned (pre-processed) review text\n",
    "def get_clean_review(review):\n",
    "    # get a substring of only alphabetical characters, replace removed characters to a space\n",
    "    review = re.sub(\"[^a-zA-Z]\", \" \", review)\n",
    "\n",
    "    # lower case the characters in the review\n",
    "    review = review.lower()\n",
    "    \n",
    "    # create a suffix stripper that receives only the word stem \n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    # remove irrelevant words (such as prepositions) from the review\n",
    "    review = review.split()\n",
    "    clean_review = list()\n",
    "    for word in review:\n",
    "        # convert the stopwords list into a set for O(1) get time complexity\n",
    "        stopwords_set = set(stopwords.words(\"english\"))\n",
    "        \n",
    "        # if its not a stopword, add the word into the cleaned review\n",
    "        if not word in stopwords_set:\n",
    "            stripped_word = ps.stem(word)\n",
    "            clean_review.append(stripped_word)\n",
    "            \n",
    "    # join the clean review list into a string with a space per each element in the list\n",
    "    space = \" \"\n",
    "    clean_review = space.join(clean_review)\n",
    "    return clean_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus is a term in NLP that just refers to a text such as a document, HTML page, etc.\n",
    "corpus = []\n",
    "\n",
    "# iterate through rows in the reviews DataFrame\n",
    "rows = reviews.shape[0]\n",
    "for row in range(0, rows):\n",
    "    review = reviews[\"Review\"][row]\n",
    "    \n",
    "    # clean the review and append it into the corpus\n",
    "    corpus.append(get_clean_review(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Matrix\n",
    "Now we need to program the bag of words sparse matrix because we pre-processed the texts in the reviews DataFrame.\n",
    "\n",
    "The matrix is structured as so: every word in the corpus has its individual column and each row is a review.\n",
    "\n",
    "Therefore, the classification model can classify each review as either positive or negative because it can quantify the frequency of each word per review and use that for classifying the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the word counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\"\"\"\n",
    "create a word counter object.\n",
    "- max_features = 1500 use only the top 1500 words (columns)\n",
    "\n",
    "This class has built-in text pre-processing parameters (such as lowercase, stopwords, etc.)\n",
    "However, we decided to program the pre-processing manually above because it gives us more control.\n",
    "Therefore, we're not going to use them since we already programmed the pre-processing above.\n",
    "\"\"\"\n",
    "max_top_words = 1500\n",
    "cv = CountVectorizer(max_features=max_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x is each word from the sparse matrix where the value is the frequency of the word\n",
    "x = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    0\n",
       "3    1\n",
       "4    1\n",
       "Name: Liked, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y is the Liked column from the original reviews data set\n",
    "y = reviews.iloc[:, 1]\n",
    "\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Bag of Words Model\n",
    "Now that we have our sparse matrix, we can fit this into a classification model and use it to predict a testing set.\n",
    "\n",
    "We will use a Naive Bayes (probability) classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data set into training and testing data sets\n",
    "from sklearn.model_selection import train_test_split \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the gaussian naive bayes class\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a naive bayes classifier, then fit to the training set\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the training set results\n",
    "y_pred = classifier.predict(x_test)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the confusion matrix function\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[55, 42],\n",
       "       [12, 91]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a confusion matrix that compares the y_test (actual) to the y_pred (prediction)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\"\"\"\n",
    "Read the Confusion Matrix diagonally:\n",
    "55 + 91 = 146 correct predictions\n",
    "42 + 12 = 54 incorrect predictions\n",
    "\"\"\"\n",
    "cm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
