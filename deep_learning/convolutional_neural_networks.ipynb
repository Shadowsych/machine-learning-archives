{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "Our brain classifies visuals by looking at the features of the object, which is why optical illusions occur. Convolutional Neural Networks are a type of neural network that filter the features of a sample, typically an image, and classify the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution\n",
    "In a convolutional operation, there are 3 components:\n",
    "- Input\n",
    "- Feature Detector (also called Filter or Kernel)\n",
    "- Feature Map (also called a Convolved Feature or Activation Map)\n",
    "\n",
    "The feature detector is a matrix that the convolutional operation uses to detect a feature on the input. Some examples of a feature could be a long nose, green eyes, long eyelashes, etc. that you could create a matrix from and use it to detect on the input.\n",
    "\n",
    "### Example of Convolutional Operation\n",
    "Let's say we gray-scale an image of a smiley face, we can represent it as a 2D matrix below.\n",
    "\n",
    "<img src=\"images/cnn/smiley_example.png\" height=\"65%\" width=\"65%\"></img>\n",
    "\n",
    "Now we can use a 3x3 (standard size) feature detector to match 3x3 sections of the image on a feature map. Let's fill the feature map below!\n",
    "\n",
    "<hr>\n",
    "\n",
    "<img src=\"images/cnn/convolutional_operation_1.png\" height=\"65%\" width=\"65%\"></img>\n",
    "\n",
    "No cells in the section and the feature detector matched, so place a 0. Let's continue with the next stride.\n",
    "\n",
    "<hr>\n",
    "\n",
    "<img src=\"images/cnn/convolutional_operation_2.png\" height=\"65%\" width=\"65%\"></img>\n",
    "\n",
    "Only the middle-left cell of the section matched the middle-left cell of the feature detector, so place a 1. Keep continuing the strides until the feature map is filled.\n",
    "\n",
    "<hr>\n",
    "\n",
    "<img src=\"images/cnn/convolutional_operation_3.png\" height=\"65%\" width=\"65%\"></img>\n",
    "\n",
    "This is the completed feature map after performing all the strides. The feature map is smaller than the image, which will be more efficient for the neural network because we only care about this certain feature and not the entire input.\n",
    "\n",
    "### Convolutional Layer\n",
    "<img src=\"images/cnn/convolutional_layer.png\" height=\"65%\" width=\"65%\"></img>\n",
    "\n",
    "We create many feature maps to create the convolutional layer, which detects all of the features on the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU Layer\n",
    "The ReLU layer is the Rectifier Activation Function. It decreases the linearity of the neural network, which helps with seeing abrupt changes in the image.\n",
    "\n",
    "<img src=\"images/cnn/relu_layer.png\" height=\"65%\" width=\"65%\"></img>\n",
    "\n",
    "### Example of ReLU Layer\n",
    "<img src=\"images/cnn/fergus.png\" height=\"40%\" width=\"40%\"></img>\n",
    "\n",
    "This is the original image.\n",
    "\n",
    "<hr>\n",
    "\n",
    "<img src=\"images/cnn/edge_fergus.png\" height=\"40%\" width=\"40%\"></img>\n",
    "\n",
    "This is the original image applied with an edge detector, a type of feature detector that detects edges on an image.\n",
    "\n",
    "<hr>\n",
    "\n",
    "<img src=\"images/cnn/relu_fergus.png\" height=\"40%\" width=\"40%\"></img>\n",
    "\n",
    "This is the edge-detected image with the applied ReLU layer. The abrupt changes in the image are seen with the ReLU layer because the black lines are zeroed out and only the white lines appear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max Pooling\n",
    "There are lots of differences among images due to space, even if they contain the same object.\n",
    "\n",
    "<img src=\"images/cnn/spatial_variance.png\" height=\"40%\" width=\"40%\"></img>\n",
    "\n",
    "For example, the three images of the cheetah above are still images of a cheetah. However, the neural network may not notice that because of the spatial variance.\n",
    "\n",
    "In order to resolve this issue, we need to implement \"spatial invariance\". Spatial invariance disregards if the features on an object are a bit distorted relative to each other.\n",
    "\n",
    "Pooling is an algorithm to achieve \"spatial invariance\". We will implement \"max\" pooling, which only receives the maximum value in a section of cells in the feature map.\n",
    "\n",
    "### Example of Max Pooling\n",
    "<img src=\"images/cnn/max_pooling_1.png\" height=\"60%\" width=\"60%\"></img>\n",
    "\n",
    "This is the first stride of the pooled feature map. The top-left cell is 1 because the maximum value in the section is 1.\n",
    "\n",
    "<hr>\n",
    "\n",
    "<img src=\"images/cnn/max_pooling_2.png\" height=\"60%\" width=\"60%\"></img>\n",
    "\n",
    "This is the completed pooled feature map after performing all the strides. The pooled feature map is smaller than the feature map, which will be even more efficient for the neural network to learn.\n",
    "\n",
    "### Effectiveness of Max Pooling\n",
    "Let's give an see how max pooling is effective to solving distortions.\n",
    "\n",
    "For instance, say the cell with value 4 on the feature map above was located on a slightly different cell location due to image distortion. The pooled feature map would still put a 4 on the middle-left because it's the maximum of the section.\n",
    "\n",
    "### Pooling Layer\n",
    "<img src=\"images/cnn/pooling_layer.png\" height=\"65%\" width=\"65%\"></img>\n",
    "\n",
    "We create many pooled feature maps to create the pooling layer, which detects all of the pooled features on the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flattening\n",
    "Flattening transforms each pooled feature map into a large single column. This column becomes the input layer of the neural network.\n",
    "\n",
    "### Example of Flattening\n",
    "<img src=\"images/cnn/flattening.png\" height=\"65%\" width=\"65%\"></img>\n",
    "\n",
    "### Input Layer\n",
    "<img src=\"images/cnn/input_layer.png\" height=\"65%\" width=\"65%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Connection\n",
    "Let's put the input layer into a fully connected neural network.\n",
    "\n",
    "<img src=\"images/cnn/full_connection.png\" height=\"65%\" width=\"65%\"></img>\n",
    "\n",
    "The layers are called \"fully connected\" because all the neurons in the layer are connected to the previous and next layers. The layers are fully connected because we want the neural network to evaluate the entire image, and not just certain features of the image.\n",
    "\n",
    "### Example of Full Connection\n",
    "<img src=\"images/cnn/dog_classify.png\" height=\"65%\" width=\"65%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax\n",
    "The Sigmoid Activation Function is typically used on a neural network with a single output neuron, but the Softmax Activation Function is best when working with multiple neurons in the output layer.\n",
    "\n",
    "### Example of Softmax\n",
    "<img src=\"images/cnn/softmax.png\" height=\"75%\" width=\"75%\"></img>\n",
    "\n",
    "If we didn't use the Softmax function, then the neural network may state the raw outputs like the image being 85% Dog and 40% Cat, which does not sum to 100%.\n",
    "\n",
    "The neurons in the output layer are not connected between each other, so how does the neural network know that the image is 95% a Dog and 5% a Cat, which sums to 100%?\n",
    "\n",
    "The Softmax function uses the normalized value of the output value and divides it by the normalized sum of the all the other output values. This is how the percentage of the outputs sum to 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Entropy\n",
    "The Cross-Entropy cost function is best when working with Softmax Activation Function. This is because it minimizes the probability of the output to the actual probability.\n",
    "\n",
    "Cross-Entropy is called a \"Loss function\" not a \"Cost function\". This is because a Cost function is just the average of the Loss functions, but Cross-Entropy doesn't calculate any averages.\n",
    "\n",
    "Cross-Entropy is best for classification because it understands that even small adjustments can decrease the error because the function is logarithmic. However, in Mean Squared Error (MSE), tiny adjustments might not change the error because MSE only compares averages.\n",
    "\n",
    "### Example of Cross-Entropy\n",
    "<img src=\"images/cnn/cross_entropy.png\" height=\"75%\" width=\"75%\"></img>\n",
    "- 1 is the actual value of a dog\n",
    "- 0 is the actual value of a cat\n",
    "- p(x) is the probability of x\n",
    "- q(x) is the normal distribution function\n",
    "\n",
    "The predicted values (0.9 and 0.1) and the actual values (1 and 0) are summed in the function.\n",
    "\n",
    "### Example of Comparing Errors\n",
    "<img src=\"images/cnn/comparing_errors.png\" height=\"75%\" width=\"75%\"></img>\n",
    "\n",
    "The diagram above shows the errors of 3 popular error estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
