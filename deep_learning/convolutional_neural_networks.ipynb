{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "Our brain classifies visuals by looking at the features of the object, which is why optical illusions occur. Convolutional Neural Networks are a type of neural network that filter the features of a sample, typically an image, and classify the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution\n",
    "In a convolutional operation, there are 3 components:\n",
    "- Input\n",
    "- Feature Detector (also called Filter or Kernel)\n",
    "- Feature Map (also called a Convolved Feature or Activation Map)\n",
    "\n",
    "The feature detector is a matrix that the convolutional operation uses to detect a feature on the input. Some examples of a feature could be a long nose, green eyes, long eyelashes, etc. that you could create a matrix from and use it to detect on the input.\n",
    "\n",
    "### Example of Convolutional Operation\n",
    "Let's say we gray-scale an image of a smiley face, we can represent it as a 2D matrix below.\n",
    "\n",
    "<img src=\"images/cnn/smiley_example.png\" height=\"65%\" width=\"65%\"></img>\n",
    "\n",
    "Now we can use a 3x3 (standard size) feature detector to match 3x3 sections of the image on a feature map. Let's fill the feature map below!\n",
    "\n",
    "<hr>\n",
    "\n",
    "<img src=\"images/cnn/convolutional_operation_1.png\" height=\"65%\" width=\"65%\"></img>\n",
    "\n",
    "No cells in the section and the feature detector matched, so place a 0. Let's continue with the next stride.\n",
    "\n",
    "<hr>\n",
    "\n",
    "<img src=\"images/cnn/convolutional_operation_2.png\" height=\"65%\" width=\"65%\"></img>\n",
    "\n",
    "Only the middle-left cell of the section matched the middle-left cell of the feature detector, so place a 1. Keep continuing the strides until the feature map is filled.\n",
    "\n",
    "<hr>\n",
    "\n",
    "<img src=\"images/cnn/convolutional_operation_3.png\" height=\"65%\" width=\"65%\"></img>\n",
    "\n",
    "This is the completed feature map after performing all the strides. The feature map is smaller than the image, which will be more efficient for the neural network because we only care about this certain feature and not the entire input.\n",
    "\n",
    "### Convolutional Layer\n",
    "<img src=\"images/cnn/convolutional_layer.png\" height=\"65%\" width=\"65%\"></img>\n",
    "\n",
    "We create many feature maps to create the convolutional hidden layer, which detects all of the features on the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU Layer\n",
    "The ReLU hidden layer is the Rectifier Activation Function. It decreases the linearity of the neural network, which helps with seeing abrupt changes in the image.\n",
    "\n",
    "<img src=\"images/cnn/relu_layer.png\" height=\"65%\" width=\"65%\"></img>\n",
    "\n",
    "### Example of ReLU Layer\n",
    "<img src=\"images/cnn/fergus.png\" height=\"40%\" width=\"40%\"></img>\n",
    "\n",
    "This is the original image.\n",
    "\n",
    "<hr>\n",
    "\n",
    "<img src=\"images/cnn/edge_fergus.png\" height=\"40%\" width=\"40%\"></img>\n",
    "\n",
    "This is the original image applied with an edge detector, a type of feature detector that detects edges on an image.\n",
    "\n",
    "<hr>\n",
    "\n",
    "<img src=\"images/cnn/relu_fergus.png\" height=\"40%\" width=\"40%\"></img>\n",
    "\n",
    "This is the edge-detected image with the applied ReLU layer. The abrupt changes in the image are seen with the ReLU layer because the black lines are zeroed out and only the white lines appear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max Pooling\n",
    "There are lots of differences among images due to space, even if they contain the same object.\n",
    "\n",
    "<img src=\"images/cnn/spatial_variance.png\" height=\"40%\" width=\"40%\"></img>\n",
    "\n",
    "For example, the three images of the cheetah above are still images of a cheetah. However, the neural network may not notice that because of the spatial variance.\n",
    "\n",
    "In order to resolve this issue, we need to implement \"spatial invariance\". Spatial invariance disregards if the features on an object are a bit distorted relative to each other.\n",
    "\n",
    "Pooling is a hidden layer to achieve \"spatial invariance\". We will implement \"max\" pooling, which only receives the maximum value in a section of cells in the feature map.\n",
    "\n",
    "### Example of Max Pooling\n",
    "<img src=\"images/cnn/max_pooling_1.png\" height=\"60%\" width=\"60%\"></img>\n",
    "\n",
    "This is the first stride of the pooled feature map. The top-left cell is 1 because the maximum value in the section is 1.\n",
    "\n",
    "<hr>\n",
    "\n",
    "<img src=\"images/cnn/max_pooling_2.png\" height=\"60%\" width=\"60%\"></img>\n",
    "\n",
    "This is the completed pooled feature map after performing all the strides. The pooled feature map is smaller than the feature map, which will be even more efficient for the neural network to learn.\n",
    "\n",
    "### Effectiveness of Max Pooling\n",
    "Let's give an see how max pooling is effective to solving distortions.\n",
    "\n",
    "For instance, say the cell with value 4 on the feature map above was located on a slightly different cell location due to image distortion. The pooled feature map would still put a 4 on the middle-left because it's the maximum of the section.\n",
    "\n",
    "### Pooling Layer\n",
    "<img src=\"images/cnn/pooling_layer.png\" height=\"65%\" width=\"65%\"></img>\n",
    "\n",
    "We create many pooled feature maps to create the pooling layer, which detects all of the pooled features on the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flattening\n",
    "Flattening is a hidden layer that transforms each pooled feature map into a large single column. This column becomes the input layer of the neural network.\n",
    "\n",
    "### Example of Flattening\n",
    "<img src=\"images/cnn/flattening.png\" height=\"65%\" width=\"65%\"></img>\n",
    "\n",
    "### Input Layer\n",
    "<img src=\"images/cnn/input_layer.png\" height=\"65%\" width=\"65%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Connection (Densely Connected)\n",
    "The last hidden layers of the CNN are the fully connected layers, also called \"Dense\" layers.\n",
    "\n",
    "<img src=\"images/cnn/full_connection.png\" height=\"65%\" width=\"65%\"></img>\n",
    "\n",
    "The layers are called \"fully connected\" or \"dense\" because all the neurons in the layer are connected to the previous and next layers. The layers are fully connected because we want the neural network to evaluate the entire image, and not just certain features of the image. However, this can come at a risk of overfitting the neural network to the training set.\n",
    "\n",
    "### Example of Full Connection\n",
    "<img src=\"images/cnn/dog_classify.png\" height=\"65%\" width=\"65%\"></img>\n",
    "\n",
    "Even though the neural network is fully connected, there are still weights on the synapses (signals). Therefore, as seen on the diagram above, only certain neurons from the last fully connected layer have a significant influence on the Dog and Cat output neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax\n",
    "The Sigmoid Activation Function is typically used on a neural network with a single output neuron, but the Softmax Activation Function is best when working with multiple neurons in the output layer.\n",
    "\n",
    "### Example of Softmax\n",
    "<img src=\"images/cnn/softmax.png\" height=\"75%\" width=\"75%\"></img>\n",
    "\n",
    "If we didn't use the Softmax function, then the neural network may state the raw outputs like the image being 85% Dog and 40% Cat, which does not sum to 100%.\n",
    "\n",
    "The neurons in the output layer are not connected between each other, so how does the neural network know that the image is 95% a Dog and 5% a Cat, which sums to 100%?\n",
    "\n",
    "The Softmax function uses the normalized value of the output value and divides it by the normalized sum of the all the other output values. This is how the percentage of the outputs sum to 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Entropy\n",
    "The Cross-Entropy cost function is best when working with Softmax Activation Function. This is because it minimizes the probability of the output to the actual probability.\n",
    "\n",
    "Cross-Entropy is called a \"Loss function\" not a \"Cost function\". This is because a Cost function is just the average of the Loss functions, but Cross-Entropy doesn't calculate any averages.\n",
    "\n",
    "Cross-Entropy is best for classification because it understands that even small adjustments can decrease the error because the function is logarithmic. However, in Mean Squared Error (MSE), tiny adjustments might not change the error because MSE only compares averages.\n",
    "\n",
    "### Example of Cross-Entropy\n",
    "<img src=\"images/cnn/cross_entropy.png\" height=\"75%\" width=\"75%\"></img>\n",
    "- 1 is the actual value of a dog\n",
    "- 0 is the actual value of a cat\n",
    "- p(x) is the probability of x\n",
    "- q(x) is the normal distribution function\n",
    "\n",
    "The predicted values (0.9 and 0.1) and the actual values (1 and 0) are summed in the function.\n",
    "\n",
    "### Example of Comparing Errors\n",
    "<img src=\"images/cnn/comparing_errors.png\" height=\"75%\" width=\"75%\"></img>\n",
    "\n",
    "The diagram above shows the errors of 3 popular error estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Augmentation\n",
    "Image augmentation allows us to create many batches of the images, which create many more diverse set of the images. Some augmentations could be rotating, stretching, zooming, etc.\n",
    "\n",
    "This helps prevent overfitting because augmentation better diversifies the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import the image data augmentor\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data generator to randomly rescale, shear (rotate), zoom, and flip images\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data generator to randomly rescale the images\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 114 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "get the training set set from the images directory \n",
    "- target_size = (64, 64) means to set all image sizes to 64 width and 64 height\n",
    "- batch_size = 32 means to separate all of the images to 32 images per batch,\n",
    "    and after each batch the neural network updates its weights\n",
    "- class_mode = categorical because we're using a softmax function for the output layer\n",
    "\"\"\"\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    \"datasets/cnn/training_set\",\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 59 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "get the testing set set from the images directory \n",
    "- target_size = (64, 64) means to set all image sizes to 64 width and 64 height\n",
    "- batch_size = 32 means to separate all of the images to 32 images per batch\n",
    "- class_mode = categorical because we're using a softmax function for the output layer\n",
    "\"\"\"\n",
    "testing_set = train_datagen.flow_from_directory(\n",
    "    \"datasets/cnn/testing_set\",\n",
    "     target_size=(64, 64),\n",
    "     batch_size=32,\n",
    "     class_mode=\"categorical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras packages to make the CNN to classify 2D images (width and height)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the sequential neural network\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pravat/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "add the convolutional layer\n",
    "- filters = 32 means use 32 feature detectors\n",
    "- kernel_size = (3, 3) means use a 3x3 feature detector matrix\n",
    "- input_shape = (64, 64, 3) means the input images are 64x64 size as RGB (colored) 3D Arrays.\n",
    "    if using black and white colors the parameter should equal to (64, 64, 1)\n",
    "- activation = relu means to use the Rectifier activation function\n",
    "\"\"\"\n",
    "classifier.add(\n",
    "    Convolution2D(\n",
    "        filters=32,\n",
    "        kernel_size=(3,3),\n",
    "        input_shape=(64, 64, 3),\n",
    "        activation=\"relu\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the max pooling layer as a 2x2 matrix\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a second convolutional layer with the same parameters to use on the first max pooled layer\n",
    "classifier.add(\n",
    "    Convolution2D(\n",
    "        filters=32,\n",
    "        kernel_size=(3,3),\n",
    "        activation=\"relu\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# add the second max pooling layer to use on the second convolutional layer\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the flattening layer\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "add the fully connected layer\n",
    "- units = 128 means use 128 neurons, this number was calculated through parameter tuning\n",
    "- activation = relu means to use the Rectifier activation function\n",
    "\"\"\"\n",
    "classifier.add(Dense(units=128, activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "add the output layer\n",
    "- units = 2 for the Dog and Cat output neurons\n",
    "- activation = softmax because there are multiple outputs\n",
    "\"\"\"\n",
    "classifier.add(Dense(units=2, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "compile the ANN\n",
    "- optimizer = adam is a specific type of Stochastic Gradient Descent algorithm\n",
    "- loss = categorical_crossentropy because we're using a softmax function for the output layer\n",
    "- metric = accuracy means to use the accuracy metric to determine how accurate the model is\n",
    "\"\"\"\n",
    "classifier.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pravat/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1\n",
      "114/114 [==============================] - 20s 179ms/step - loss: 0.5358 - acc: 0.7093 - val_loss: 0.9848 - val_acc: 0.5582\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5b648903c8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "fit the data generator to the training set\n",
    "- steps_per_epoch = 114 means there are 114 training images (steps), which equals 1 epoch\n",
    "- epochs = 1 means to only use 1 epoch (we're using a small epoch to save time)\n",
    "- validation_data = testing_set means to use the testing set to determine the accuracy\n",
    "- validation_steps = 59 means there are 59 testing images\n",
    "\n",
    "For the first epoch, we can see the acc = 0.7093 (testing set accuracy), and the\n",
    "val_acc = 0.5582(training set accuracy). This seems like overfitting to the training set.\n",
    "\n",
    "A reason for overfitting could be because we used 2 convolutional layers.\n",
    "This is because having too many layers can lead to an overfitting of the\n",
    "training set because more layers creates more neurons that look for more\n",
    "specific features on the training images.\n",
    "\n",
    "Another reason for overfitting could be because its a fully-connected (Dense) neural network.\n",
    "\"\"\"\n",
    "classifier.fit_generator(\n",
    "    training_set,\n",
    "    steps_per_epoch=114,\n",
    "    epochs=1,\n",
    "    validation_data=testing_set,\n",
    "    validation_steps=59\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
