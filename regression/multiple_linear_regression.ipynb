{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "The same as simple linear regression, but with multiple x-variables (independent variables).\n",
    "\n",
    "### Formula\n",
    "```y = m1*x1 + m2*x2 + ... mn*xn + b```\n",
    "- n = number of x-variables\n",
    "\n",
    "### 3D Visual Example\n",
    "<img src=\"images/mlr/mlr_example.png\" height=\"60%\" width=\"60%\"></img>\n",
    "\n",
    "https://towardsdatascience.com/graphs-and-ml-multiple-linear-regression-c6920a1f2e70\n",
    "\n",
    "- The two independent variables are Weight and Horsepower\n",
    "    - It's important to notice that these variables are independent from each other\n",
    "- The dependent variable is MPG\n",
    "\n",
    "As a result of the linear regression, a hyper-plane is created in 3D space.\n",
    "- The hyper-plane was created through the OLS (Ordinary Least Squares) method\n",
    "\n",
    "### Feature Scaling? Not Needed in Regression!\n",
    "Feature scaling is not needed for even multi-variate linear regression models. This is because the dependent variable is a combination of the independent variables, so the coefficients (slopes) of each independent variable would adopt a scale to put everything on the same scale.\n",
    "\n",
    "For example, ```y = 0.5x```. If y = 1000 when x = 2000, then the coefficient (slope) equals 0.5 to scale the result properly. This same concept applies in multi-variate linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Variables\n",
    "Sometimes variables are non-numeric and show categorical information. These variables/columns would require dummy variables to use within a mathematical equation like Multiple Linear Regression.\n",
    "\n",
    "<img src=\"images/mlr/dummy_variables.png\" height=\"75%\" width=\"75%\"></img>\n",
    "\n",
    "In the example above, the \"State\" column is a categorical variable, thus it needs dummy variables.\n",
    "- For each unique value in the State column, create a Dummy variable/column that indicates if the category was present or not present at the row\n",
    "\n",
    "### Dummy Variable Trap\n",
    "In the example, we crossed-out the \"California\" dummy variable. But why?\n",
    "\n",
    "In a multi-variate model, it's necessary to always omit one dummy variable.\n",
    "- This is because there's always a remaining duplicate variable\n",
    "\n",
    "The dummy variable trap causes perfect multicollinearity (highly correlated variables), which would scew the regression model. Perfect multicollinearity is when a variable can determine the value of another variable, so one predictor variable can be used to predict another. This creates redundancy, which skews the results of a regression model.\n",
    "\n",
    "### Handling Dummy Variable Trap\n",
    "Fortunately, SKLearn automatically removes highly correlated variables (by calculating correlation coefficients), so the dummy variable trap would be automatically resolved by SKLearn.\n",
    "\n",
    "However, other statistical models (such as the statsmodels API) may not handle the trap. To handle the trap, just omit a single dummy variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Omitting Variables (Model Building)\n",
    "Sometimes variables must be thrown-out because they're \"garbage-in\" data that would lead to \"garbage-out\" predictions.\n",
    "\n",
    "We don't want this garbage data because they might not predict anything useful for the model.\n",
    "\n",
    "### P-Values\n",
    "Tells us how likely it is to get a result like this if the Null Hypothesis is true.\n",
    "\n",
    "P-values can determine if an outcome is statistically significant.\n",
    "- Lower p-values means that the outcome is significant to study\n",
    "- Higher p-values means that the outcome was out of luck, and not significant to study\n",
    "\n",
    "By convention, the signifiance level (SL) is usually 0.05:    \n",
    "If p < 0.05, then we reject the null hypothesis.  \n",
    "If p >= 0.05, then we accept null hypothesis.\n",
    "\n",
    "### 5 Methods of Building Models\n",
    "1. All-in: Use all the variables\n",
    "\n",
    "2. Backward Elimination\n",
    "    - Step 1: Select a signifiance level (0.05) to stay in the model\n",
    "    - Step 2: Fit the full model with all possible predictors from Step 1\n",
    "    - Step 3: Consider the predictor with the highest P-value.\n",
    "        - If P > 0.05, go to Step 4\n",
    "        - Otherwise, the model is ready\n",
    "    - Step 4: Remove the predictor and re-fit the model without the predictor. Then go to Step 3.\n",
    "    \n",
    "\n",
    "3. Forward Selection\n",
    "    - Step 1: Select a signifiance level (0.05) to enter in the model\n",
    "    - Step 2: Fit ALL possible regression models, ```y ~ xn```. Select the one with lowest P-value.\n",
    "    - Step 3: Keep this variable, then fit all possible models an extra predictor added to the one(s) you kept\n",
    "    - Step 4: Consider the predictor with the lowest P-value.\n",
    "        - If P < 0.05, go to Step 3\n",
    "        - Otherwise, the model is ready\n",
    "        \n",
    "\n",
    "4. Bidirectional Elimination (Stepwise Regression)\n",
    "    - Step 1: Select a significance level (0.05) to enter and to stay in the model\n",
    "    - Step 2: Perform thenext step of forward selection \n",
    "        - New variables must have P < 0.05 to enter\n",
    "    - Step 3: Perform Backward Elimination\n",
    "        - Old variables must have P < 0.05 to stay\n",
    "        - Now go back, and repeat Step 3\n",
    "    - Step 4: No new variables can enter and no old variables can exit. The model is ready.\n",
    "\n",
    "\n",
    "5. Score Comparision (All Possible Models)\n",
    "    - Step 1: Select a criteriion of goodness of fit.\n",
    "        - Ex: Akaike Criterion\n",
    "    - Step 2: Construct all possible regression models\n",
    "        - 2^N - 1 total combinations\n",
    "    - Step 3: Select the one with the best criterion. Then your model is ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLearn Multi-Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R&amp;D Spend</th>\n",
       "      <th>Administration</th>\n",
       "      <th>Marketing Spend</th>\n",
       "      <th>State</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165349.20</td>\n",
       "      <td>136897.80</td>\n",
       "      <td>471784.10</td>\n",
       "      <td>New York</td>\n",
       "      <td>192261.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>162597.70</td>\n",
       "      <td>151377.59</td>\n",
       "      <td>443898.53</td>\n",
       "      <td>California</td>\n",
       "      <td>191792.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153441.51</td>\n",
       "      <td>101145.55</td>\n",
       "      <td>407934.54</td>\n",
       "      <td>Florida</td>\n",
       "      <td>191050.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>144372.41</td>\n",
       "      <td>118671.85</td>\n",
       "      <td>383199.62</td>\n",
       "      <td>New York</td>\n",
       "      <td>182901.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142107.34</td>\n",
       "      <td>91391.77</td>\n",
       "      <td>366168.42</td>\n",
       "      <td>Florida</td>\n",
       "      <td>166187.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   R&D Spend  Administration  Marketing Spend       State     Profit\n",
       "0  165349.20       136897.80        471784.10    New York  192261.83\n",
       "1  162597.70       151377.59        443898.53  California  191792.06\n",
       "2  153441.51       101145.55        407934.54     Florida  191050.39\n",
       "3  144372.41       118671.85        383199.62    New York  182901.99\n",
       "4  142107.34        91391.77        366168.42     Florida  166187.94"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the data set\n",
    "salary_df = pd.read_csv(\"datasets/50_startups.csv\")\n",
    "\n",
    "salary_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "x is the R&D Spend, Administration, Marketing Spend, and State columns.\n",
    "Because the State is a categorical variable, we must encode it using OneHotEncoder.\n",
    "\"\"\"\n",
    "x = salary_df.iloc[:, :-1].values\n",
    "\n",
    "# encode the State column; no need to use LabelEncoder with latest SKLearn for OneHotEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "dummy_transformer = make_column_transformer((OneHotEncoder(), [3]),\n",
    "                                            remainder=\"passthrough\")\n",
    "x = dummy_transformer.fit_transform(x)\n",
    "\n",
    "# avoid the dummy variable trap; optional because SKLearn handles this automatically\n",
    "x = x[:, 1:]\n",
    "\n",
    "# y is the Profit column\n",
    "y = salary_df.iloc[:, 4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data set into training and testing data sets\n",
    "from sklearn.model_selection import train_test_split \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the linear regression class\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# create a regressor model\n",
    "regressor = LinearRegression()\n",
    "\n",
    "# fit the training data, feature scaling is not needed for regression models\n",
    "regressor.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([103015.20159796, 132582.27760815, 132447.73845175,  71976.09851258,\n",
       "       178537.48221056, 116161.24230166,  67851.69209676,  98791.73374687,\n",
       "       113969.43533013, 167921.06569551])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the test set results, feature scaling is not needed for regression models\n",
    "y_pred = regressor.predict(x_test)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 103015.0 vs Actual: 103282.38 ---> Difference: 267.38000000000466\n",
      "Predicted: 132582.0 vs Actual: 144259.4 ---> Difference: 11677.399999999994\n",
      "Predicted: 132448.0 vs Actual: 146121.95 ---> Difference: 13673.950000000012\n",
      "Predicted: 71976.0 vs Actual: 77798.83 ---> Difference: 5822.830000000002\n",
      "Predicted: 178537.0 vs Actual: 191050.39 ---> Difference: 12513.390000000014\n",
      "Predicted: 116161.0 vs Actual: 105008.31 ---> Difference: 11152.690000000002\n",
      "Predicted: 67852.0 vs Actual: 81229.06 ---> Difference: 13377.059999999998\n",
      "Predicted: 98792.0 vs Actual: 97483.56 ---> Difference: 1308.4400000000023\n",
      "Predicted: 113969.0 vs Actual: 110352.25 ---> Difference: 3616.75\n",
      "Predicted: 167921.0 vs Actual: 166187.94 ---> Difference: 1733.0599999999977\n"
     ]
    }
   ],
   "source": [
    "# compare y_pred (prediction) to the y_test (actual)\n",
    "i = 0\n",
    "while i < len(y_pred):\n",
    "    diff = abs(round(y_pred[i]) - y_test[i])\n",
    "    print(\"Predicted: \" + str(round(y_pred[i])) + \" vs Actual: \" + str(y_test[i]) +\n",
    "          \" ---> Difference: \" + str(diff))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Building The Model with Backward Elimination\n",
    "There might've been garbage variables that scewed the predictions. We can attempt to remove these garbage variables through a backward elimination process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a stats model (sm)\n",
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "add a column of 1s as the first column of x.\n",
    "this column is represented as b0 for the stats model to perform the backward elimination.\n",
    "\"\"\"\n",
    "fifty_ones = np.ones((50, 1)).astype(int)\n",
    "x = np.append(fifty_ones, x, axis=1)\n",
    "\n",
    "# deep copy x onto a x_opt variable, which will contain the significant independent variables\n",
    "x_opt =  np.array(x[:, [0, 1, 2, 3, 4, 5]], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.951</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   169.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 28 May 2019</td> <th>  Prob (F-statistic):</th> <td>1.34e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:27:22</td>     <th>  Log-Likelihood:    </th> <td> -525.38</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1063.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    44</td>      <th>  BIC:               </th> <td>   1074.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 5.013e+04</td> <td> 6884.820</td> <td>    7.281</td> <td> 0.000</td> <td> 3.62e+04</td> <td>  6.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>  198.7888</td> <td> 3371.007</td> <td>    0.059</td> <td> 0.953</td> <td>-6595.030</td> <td> 6992.607</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>  -41.8870</td> <td> 3256.039</td> <td>   -0.013</td> <td> 0.990</td> <td>-6604.003</td> <td> 6520.229</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.8060</td> <td>    0.046</td> <td>   17.369</td> <td> 0.000</td> <td>    0.712</td> <td>    0.900</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   -0.0270</td> <td>    0.052</td> <td>   -0.517</td> <td> 0.608</td> <td>   -0.132</td> <td>    0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>    0.0270</td> <td>    0.017</td> <td>    1.574</td> <td> 0.123</td> <td>   -0.008</td> <td>    0.062</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.782</td> <th>  Durbin-Watson:     </th> <td>   1.283</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.948</td> <th>  Prob(JB):          </th> <td>2.41e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.572</td> <th>  Cond. No.          </th> <td>1.45e+06</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.45e+06. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.951\n",
       "Model:                            OLS   Adj. R-squared:                  0.945\n",
       "Method:                 Least Squares   F-statistic:                     169.9\n",
       "Date:                Tue, 28 May 2019   Prob (F-statistic):           1.34e-27\n",
       "Time:                        12:27:22   Log-Likelihood:                -525.38\n",
       "No. Observations:                  50   AIC:                             1063.\n",
       "Df Residuals:                      44   BIC:                             1074.\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       5.013e+04   6884.820      7.281      0.000    3.62e+04     6.4e+04\n",
       "x1           198.7888   3371.007      0.059      0.953   -6595.030    6992.607\n",
       "x2           -41.8870   3256.039     -0.013      0.990   -6604.003    6520.229\n",
       "x3             0.8060      0.046     17.369      0.000       0.712       0.900\n",
       "x4            -0.0270      0.052     -0.517      0.608      -0.132       0.078\n",
       "x5             0.0270      0.017      1.574      0.123      -0.008       0.062\n",
       "==============================================================================\n",
       "Omnibus:                       14.782   Durbin-Watson:                   1.283\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.266\n",
       "Skew:                          -0.948   Prob(JB):                     2.41e-05\n",
       "Kurtosis:                       5.572   Cond. No.                     1.45e+06\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.45e+06. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Step 2) create an OLS regressor to fit the model with all possible predictors\n",
    "regressor_OLS = sm.OLS(endog=y, exog=x_opt).fit()\n",
    "\n",
    "\"\"\"\n",
    "Reading the Summary, we notice:\n",
    "- const (index 0) has a p-value of 0.000\n",
    "- x1 (index 1) has a p-value of 0.000\n",
    "- x2 (index 2) has a p-value of 0.000\n",
    "- x3 (index 3) has a p-value of 0.732\n",
    "- x4 (index 4) has a p-value of 0.003\n",
    "- x5 (index 5) has a p-value of 0.030\n",
    "\n",
    "We can remove the x3 and x4 columns because it has p-values greater than 0.05.\n",
    "\"\"\"\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.562</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.534</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   19.71</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 28 May 2019</td> <th>  Prob (F-statistic):</th> <td>2.32e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:27:22</td>     <th>  Log-Likelihood:    </th> <td> -579.99</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1168.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    46</td>      <th>  BIC:               </th> <td>   1176.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 5.864e+04</td> <td> 8984.018</td> <td>    6.527</td> <td> 0.000</td> <td> 4.06e+04</td> <td> 7.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>-1194.5800</td> <td> 9818.999</td> <td>   -0.122</td> <td> 0.904</td> <td> -2.1e+04</td> <td> 1.86e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td> 4196.5465</td> <td> 9467.707</td> <td>    0.443</td> <td> 0.660</td> <td>-1.49e+04</td> <td> 2.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.2480</td> <td>    0.033</td> <td>    7.525</td> <td> 0.000</td> <td>    0.182</td> <td>    0.314</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 3.720</td> <th>  Durbin-Watson:     </th> <td>   1.174</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.156</td> <th>  Jarque-Bera (JB):  </th> <td>   2.973</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.299</td> <th>  Prob(JB):          </th> <td>   0.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.034</td> <th>  Cond. No.          </th> <td>8.11e+05</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 8.11e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.562\n",
       "Model:                            OLS   Adj. R-squared:                  0.534\n",
       "Method:                 Least Squares   F-statistic:                     19.71\n",
       "Date:                Tue, 28 May 2019   Prob (F-statistic):           2.32e-08\n",
       "Time:                        12:27:22   Log-Likelihood:                -579.99\n",
       "No. Observations:                  50   AIC:                             1168.\n",
       "Df Residuals:                      46   BIC:                             1176.\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       5.864e+04   8984.018      6.527      0.000    4.06e+04    7.67e+04\n",
       "x1         -1194.5800   9818.999     -0.122      0.904    -2.1e+04    1.86e+04\n",
       "x2          4196.5465   9467.707      0.443      0.660   -1.49e+04    2.33e+04\n",
       "x3             0.2480      0.033      7.525      0.000       0.182       0.314\n",
       "==============================================================================\n",
       "Omnibus:                        3.720   Durbin-Watson:                   1.174\n",
       "Prob(Omnibus):                  0.156   Jarque-Bera (JB):                2.973\n",
       "Skew:                          -0.299   Prob(JB):                        0.226\n",
       "Kurtosis:                       4.034   Cond. No.                     8.11e+05\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 8.11e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Step 2) re-create an OLS regressor to fit the model with all possible predictors\n",
    "x_opt =  np.array(x[:, [0, 1, 2, 5]], dtype=float)\n",
    "regressor_OLS = sm.OLS(endog=y, exog=x_opt).fit()\n",
    "\n",
    "\"\"\"\n",
    "Based on the summary results, there are no more columns with a p-value greater than 0.05.\n",
    "Therefore, we finally got the significant columns among the independent variables!\n",
    "\"\"\"\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Creating The Model?\n",
    "Since we finally determined the significant independent variables, we may re-create the Multiple Linear Regression model using the ```x_opt``` variable.\n",
    "\n",
    "Although, that process is simple and redundant, so a demonstration is not needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
