{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "A technique used to solve the overfitting problem of a machine learning model.\n",
    "\n",
    "If a model learns too much, in which it learns the \"background noise\" (outliers) while being fit, then it may overfit the model to the unnecessary data points that hurts generalization.\n",
    "\n",
    "Overfitting arrises from few data points or highly correlated independent variables (high multi-collinearity).\n",
    "\n",
    "We can prevent this type of problem through regularization parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "In a Machine Learning model, if we use a simple linear regression model, then the model will use the Ordinary Least Squares method to determine the line of best-fit.\n",
    "\n",
    "However, what if there were a few background noises or not enough data points, so the model fits too much to the data?\n",
    "\n",
    "### Simple Linear vs Ridge Regression\n",
    "The Simple Linear Regression's formula is: ```y = b0 + b1*x```.\n",
    "\n",
    "Simple Linear uses the formula with the smallest value of ```squared residuals```.\n",
    "\n",
    "However, Ridge Regression uses the formula with the smallest value of:  \n",
    "```squared residuals + (lambda * b1^2)```\n",
    "- b1^2 adds a penalty to the traditional Least Squares method\n",
    "- lambda determines how severe the b1^2 penalty is\n",
    "    - The higher the lambda, the larger the squared residuals, so the higher the penalty\n",
    "    \n",
    "Lambda is considered a \"regularization\" parameter, which reduces overfitting.\n",
    "\n",
    "The ```lambda * b1^2``` penalty is called the L2 (Ridge) penalty.\n",
    "- The L2 penalty uses the sum of squared coefficients\n",
    "\n",
    "#### Ridge Regression for Multi-Variate Linear Regression\n",
    "If we were using a multi-variate linear regression model, then each b-coefficient would used in the Ridge Regression's smallest squared residuals formula:   \n",
    "```squared residuals + [lambda * (b1^2 + b2^2 + b3^2 + ... bn^2)]```\n",
    "\n",
    "An independent variable that is highly correlated to another independent variable have similarly correlating b-coefficients.\n",
    "\n",
    "This is very helpful to prevent highly correlated independent variables because the model wouldn't use an equation of large b-coefficients for highly correlated independent variables.\n",
    "- This is because the larger the b-coefficients, the greater the L2 penalty\n",
    "- Thereby, the highly correlated independent variables don't contribute as significantly to the model\n",
    "\n",
    "### Example of Overfit Simple Linear Regression\n",
    "<img src=\"images/rr/overfit_regression_example.png\" height=\"35%\" width=\"35%\"></img>\n",
    "\n",
    "This overfitting here is being caused by the low amount of training data points.\n",
    "\n",
    "The red line (regression line) is overfit to the red training data set. Therefore, the predictions made on the green testing data set would be inaccurate.\n",
    "\n",
    "Instead, we can solve this problem by using Ridge Regression that adds some bias.\n",
    "<img src=\"images/rr/ridge_regression_example.png\" height=\"35%\" width=\"35%\"></img>\n",
    "\n",
    "The blue line (ridge regression line) adds a small amount of bias, but now the line has less variance from the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression\n",
    "Lasso Regression uses the formula with the smallest value of:  \n",
    "```squared residuals + (lambda * |b1|)```\n",
    "- |b1| adds a penalty to the traditional Least Squares method\n",
    "- lambda determines how severe the |b1| penalty is\n",
    "    - The higher the lambda, the larger the squared residuals, so the higher the penalty\n",
    "    \n",
    "The ```lambda * |b1|``` penalty is called the L1 (Lasso) penalty.\n",
    "- The L1 penalty uses the absolute value of the coefficients\n",
    "    \n",
    "The multi-variate concept for Lasso Regression is the same as Ridge Regression,  but it uses the L1 penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso versus Ridge Regression?\n",
    "Ridge Regression uses the L2 (square of the coefficients) penalty.  \n",
    "Lasso Regression uses the L1 (absolute value of the coefficients) penalty.\n",
    "\n",
    "Lasso Regression can actually use an equation that zeros-out b-coefficients, which gets rid of them completely.\n",
    "\n",
    "In contrast, Ridge Regression can only ever use an equation with b-coefficients that are very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the regression models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "\n",
    "# create linear data\n",
    "x = np.array([[0], [5]])\n",
    "y = np.array([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a simple linear regressor, then fit it to the training data\n",
    "simple_regressor = LinearRegression()\n",
    "simple_regressor.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a ridge regressor with lambda = 1, then fit it to the training data\n",
    "ridge_regressor = Ridge(alpha=1)\n",
    "ridge_regressor.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a ridge regressor with lambda = 0.10, then fit it to the training data\n",
    "lasso_regressor = Lasso(alpha=0.10)\n",
    "lasso_regressor.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.]\n",
      "[2.88888889]\n",
      "[2.88]\n"
     ]
    }
   ],
   "source": [
    "# simple regressor prediction of 10\n",
    "print(simple_regressor.predict([[10]]))\n",
    "\n",
    "# ridge regressor prediction of 10\n",
    "print(ridge_regressor.predict([[10]]))\n",
    "\n",
    "# lasso regressor prediction of 10\n",
    "print(lasso_regressor.predict([[10]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
