{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "A technique used to solve the overfitting or underfitting problem of a machine learning model.\n",
    "\n",
    "If a model learns too much, in which it learns the \"background noise\" (outliers) while being fit, then it may overfit the model to the unnecessary data points that hurts generalization.\n",
    "\n",
    "We can prevent this type of problem through regularization parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "In a Machine Learning model, if we use a simple linear regression model, then the model will use the Ordinary Least Squares method to determine the line of best-fit.\n",
    "\n",
    "However, what if there were a few background noises or not enough data points, so the model fits too much to the data?\n",
    "\n",
    "### Simple Linear vs Ridge Regression\n",
    "The Simple Linear Regression's formula is: ```y = b0 + b1*x```.\n",
    "\n",
    "Simple Linear uses the formula with the smallest value of ```squared residuals```.\n",
    "\n",
    "However, Ridge Regression uses the formula with the smallest value of:  \n",
    "```squared residuals + (lambda * b1^2)```\n",
    "- b1^2 adds a penalty to the traditional Least Squares method\n",
    "- lambda determines how severe the b1^2 penalty is\n",
    "    - The higher the lambda, the larger the squared residuals, so the higher the penalty\n",
    "\n",
    "#### Ridge Regression for Multi-Variate Linear Regression\n",
    "If we were using a multi-variate linear regression model, then each b-coefficient would used in the Ridge Regression's smallest squared residuals formula:   \n",
    "```squared residuals + [lambda * (b1^2 + b2^2 + b3^2 + ... bn^2)]```\n",
    "\n",
    "### Example of Overfit Simple Linear Regression\n",
    "<img src=\"images/rr/overfit_regression_example.png\" height=\"35%\" width=\"35%\"></img>\n",
    "\n",
    "The red line (regression line) is overfit to the red training data set. Therefore, the predictions made on the green testing data set would be inaccurate.\n",
    "\n",
    "Instead, we can solve this problem by adding a \"ridge\" regularization parameter that adds some bias.\n",
    "<img src=\"images/rr/ridge_regression_example.png\" height=\"35%\" width=\"35%\"></img>\n",
    "\n",
    "The blue line (ridge regression line) adds a small amount of bias, but has less variance for the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the regression models\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "# create linear data\n",
    "x = np.array([[0], [5]])\n",
    "y = np.array([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a simple linear regressor, then fit it to the training data\n",
    "simple_regressor = LinearRegression()\n",
    "simple_regressor.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a ridge regressor with lambda = 1, then fit it to the training data\n",
    "ridge_regressor = Ridge(alpha=1)\n",
    "ridge_regressor.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.]\n",
      "[2.88888889]\n"
     ]
    }
   ],
   "source": [
    "# simple regressor prediction of 10\n",
    "print(simple_regressor.predict([[10]]))\n",
    "\n",
    "# ridge regressor prediction of 10\n",
    "print(ridge_regressor.predict([[10]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
