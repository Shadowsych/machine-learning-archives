{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "A gradient-boosting technique used to solve the overfitting problem of a machine learning model.\n",
    "\n",
    "If a model learns too much, in which it learns the \"background noise\" (outliers) while being fit, then it may overfit the model to the unnecessary data points that hurts generalization.\n",
    "\n",
    "We can prevent this type of problem through regularization parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "In a Machine Learning model, if we use a simple linear regression model, then the model will use the Ordinary Least Squares method to determine the line of best-fit.\n",
    "\n",
    "However, what if there were a few background noises or not enough data points, so the model fits too much to the data?\n",
    "\n",
    "### Simple Linear vs Ridge Regression\n",
    "The Simple Linear Regression's formula is: ```y = b0 + b1*x```.\n",
    "\n",
    "Simple Linear uses the formula with the smallest value of ```squared residuals```.\n",
    "\n",
    "However, Ridge Regression uses the formula with the smallest value of:  \n",
    "```squared residuals + (lambda * b1^2)```\n",
    "- b1^2 adds a penalty to the traditional Least Squares method\n",
    "- lambda determines how severe the b1^2 penalty is\n",
    "    - The higher the lambda, the larger the squared residuals, so the higher the penalty\n",
    "    \n",
    "Lambda is considered a \"regularization\" parameter, which reduces overfitting.\n",
    "\n",
    "The ```lambda * b1^2``` penalty is called the L2 (Ridge) penalty.\n",
    "- The L2 penalty uses the sum of squared coefficients\n",
    "\n",
    "#### Ridge Regression for Multi-Variate Linear Regression\n",
    "If we were using a multi-variate linear regression model, then the same concept applies:  \n",
    "```squared residuals + [lambda * (b1^2 + b2^2 + b3^2 + ... bn^2)]```\n",
    "\n",
    "### Example of Overfit Simple Linear Regression\n",
    "<img src=\"images/rr/overfit_regression_example.png\" height=\"35%\" width=\"35%\"></img>\n",
    "\n",
    "This overfitting here is being caused by the low amount of training data points.\n",
    "\n",
    "The red line (regression line) is overfit to the red training data set. Therefore, the predictions made on the green testing data set would be inaccurate.\n",
    "\n",
    "Instead, we can solve this problem by using Ridge Regression that adds some bias.\n",
    "<img src=\"images/rr/ridge_regression_example.png\" height=\"35%\" width=\"35%\"></img>\n",
    "\n",
    "The blue line (ridge regression line) adds a small amount of bias, but now the line has less variance from the predictions.\n",
    "\n",
    "This less variance also reduces the high variance caused by highly correlated independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression\n",
    "Lasso Regression uses the formula with the smallest value of:  \n",
    "```squared residuals + (lambda * |b1|)```\n",
    "- |b1| adds a penalty to the traditional Least Squares method\n",
    "- lambda determines how severe the |b1| penalty is\n",
    "    - The higher the lambda, the larger the squared residuals, so the higher the penalty\n",
    "    \n",
    "The ```lambda * |b1|``` penalty is called the L1 (Lasso) penalty.\n",
    "- The L1 penalty uses the absolute value of the coefficients\n",
    "    \n",
    "The multi-variate concept for Lasso Regression is the same as Ridge Regression,  but it uses the L1 penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso versus Ridge Regression?\n",
    "Ridge Regression uses the L2 (square of the coefficients) penalty.\n",
    "\n",
    "Ridge Regression helps reduce the high variance of prediction values caused by highly correlated independent variables because Ridge Regression shrinks the variables together.\n",
    "\n",
    "------\n",
    "\n",
    "Lasso Regression uses the L1 (absolute value of the coefficients) penalty.\n",
    "\n",
    "Lasso Regression helps reduce the high variance of prediction values caused by highly correlated independent variables because Lasso Regression uses one variable and zeroes-out the others.\n",
    "\n",
    "### When To Use Ridge Regression?\n",
    "Ridge Regression works best when the model contains lots of useful independent variables.\n",
    "\n",
    "### When To Use Lasso Regression?\n",
    "Lasso Regression works best when the model contains lots of useless independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ElasticNet Regression\n",
    "What if we don't know if the independent variables of my data set are useful or useless?\n",
    "\n",
    "It combines the Ridge (L2) and Lasso (L1) Regression penalties with separate Lambdas:  \n",
    "```squared residuals + (lambda1 * b1^2) + (lambda2 * |b1|)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the regression models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "\n",
    "# create linear data\n",
    "x = np.array([[0], [5]])\n",
    "y = np.array([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a simple linear regressor, then fit it to the training data\n",
    "simple_regressor = LinearRegression()\n",
    "simple_regressor.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "create a ridge regressor, then fit it to the training data\n",
    "- - alpha = 1, the constant that multiplies the penalty term\n",
    "\"\"\"\n",
    "ridge_regressor = Ridge(alpha=1)\n",
    "ridge_regressor.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "create a lasso regressor, then fit it to the training data\n",
    "- alpha = 0.1, the constant that multiplies the penalty term\n",
    "\"\"\"\n",
    "lasso_regressor = Lasso(alpha=0.10)\n",
    "lasso_regressor.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
       "      max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "create an elastic-net regressor, then fit it to the training data\n",
    "- alpha = 0.5, the constant that multiplies the penalty term\n",
    "- l1_ratio = 0.5, indicates that Lambda_1 and Lambda_2 are equivalent\n",
    "\"\"\"\n",
    "elastic_net_regressor = ElasticNet(alpha=0.5, l1_ratio=0.5)\n",
    "elastic_net_regressor.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.]\n",
      "[2.88888889]\n",
      "[2.88]\n",
      "[2.65384615]\n"
     ]
    }
   ],
   "source": [
    "# simple regressor prediction of 10\n",
    "print(simple_regressor.predict([[10]]))\n",
    "\n",
    "# ridge regressor prediction of 10\n",
    "print(ridge_regressor.predict([[10]]))\n",
    "\n",
    "# lasso regressor prediction of 10\n",
    "print(lasso_regressor.predict([[10]]))\n",
    "\n",
    "# elasticnet regressor prediction of 10\n",
    "print(elastic_net_regressor.predict([[10]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
