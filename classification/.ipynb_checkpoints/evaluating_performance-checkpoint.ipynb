{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Classification Models Performance\n",
    "There are multiple methods to evaluate classification model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# False Positive and Negatives\n",
    "We cannot always assume that the classification is correct, even for our training data set.\n",
    "\n",
    "<img src=\"images/evaluation/false_positives_negatives.png\" height=\"75%\" width=\"75%\"></img>\n",
    "- The \"red\" data points are the testing data points, classified as either 0 or 1\n",
    "- The \"blue\" data points are the predicted classifications of the red (testing) data points\n",
    "- The \"gray\" data points are the classification of the blue (predicted) data points\n",
    "\n",
    "As we can see, some of the predicted values were false for the testing set.\n",
    "- Type 1 (False Positive) Error with data point #3\n",
    "- Type 2 (False Negative) Error with data point #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix\n",
    "<img src=\"images/evaluation/confusion_matrix.png\" height=\"75%\" width=\"75%\"></img>\n",
    "\n",
    "We can model the \"False Positive and Negatives\" intuition using a Confusion Matrix.\n",
    "- 40 data points were actually 0\n",
    "    - 35 data points predicted 0\n",
    "    - 5 data points predicted 1\n",
    "- 60 data points points were actually 1\n",
    "    - 10 data points predicted 0\n",
    "    - 50 data points predicted 1\n",
    "    \n",
    "Therefore, there were 5 + 10 = 15 incorrect predictions and 35 + 50 = 85 correct predictions.\n",
    "- 85% accuracy rate\n",
    "- 15% error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Paradox\n",
    "Let's say you have a training set of 100 people, and the model predicts if they have cancer.\n",
    "- In reality, only 2 of them have cancer\n",
    "\n",
    "You create 2 classification models.\n",
    "\n",
    "### Model 1:\n",
    "Your model simply assumes nobody has cancer.\n",
    "\n",
    "You find out 2 people actually had cancer. Therefore, 2 incorrect predictions, so the accuracy of this model is 98%.\n",
    "\n",
    "### Model 2:\n",
    "You run a random forest classification, and you predict 5 people have cancer.\n",
    "\n",
    "You find out 2 people actually had cancer, and 3 people did not have cancer. Therefore, 3 incorrect predictions, so the accuracy of this model is 97%.\n",
    "\n",
    "### Comparision of Models\n",
    "Even though Model 2 is less accurate, it's a far more useful algorithm than just assuming no one has cancer.\n",
    "\n",
    "In conclusion, accuracy is not the greatest judgement of a classification model. We need to delve deeper and find out exactly what the model is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cumulative Accuracy Profile (CAP) Curve\n",
    "Based on a sample's category, we can predict that sample's classification even better than predicting its classification as if its a random category.\n",
    "\n",
    "We can see how much gain we get in each of these models compared to the random scenario.\n",
    "- Hit Ratio: Return on investment\n",
    "\n",
    "### CAP Curve Example\n",
    "Let's say there's a new video game that released, and we model the category of ages to predict if a person does or does not purchase the video game (a binary classification problem).\n",
    "- People ages 0 to 10 are more likely to purchase this game compared to random ages\n",
    "- People ages 10 to 20 are very likely to purchase this game compared to random ages\n",
    "\n",
    "<img src=\"images/evaluation/cap_model.png\" height=\"75%\" width=\"75%\"></img>\n",
    "\n",
    "The \"random\" model is of all ages.  \n",
    "The \"poor\" model is of the ages 0 to 10.  \n",
    "The \"good\" model is of the ages 10 to 20.  \n",
    "The \"crystal ball\" model is when we can see the future and predict exactly who purchases the game.\n",
    "- Beware, if we can somehow create a \"crystal ball\" model, we overfitted to the data set\n",
    "\n",
    "In the model, we can see that if we contact people from ages 10 to 20, we will have more purchases than if we contact people from ages 0 to 10. Therefore, a company may focus on only contacting people of ages 10 to 20 to maximize profits.\n",
    "\n",
    "### CAP Analysis\n",
    "We could do a method to get the area of the curves, compare them, and make a ratio. Unfortunately, this is complicated. Therefore, there's a better method to evaluating the CAP model performance.\n",
    "\n",
    "#### Look at the 50% point of the independent variable(s):\n",
    "<img src=\"images/evaluation/cap_analysis.png\" height=\"75%\" width=\"75%\"></img>\n",
    "- If Purchased < 60%, then the model is rubbish, you can make a better model\n",
    "- If 60% < Purchased < 70%, then the model is poor, you can make a better model\n",
    "- If 70% < Purchased < 80%, then the model is good\n",
    "- If 80% < Purchased < 90%, then the model is very good, but there may be overfitting to the data set\n",
    "- If 90% < Purchased , 100%, then the model is too good, but there most likely is overfitting to the data set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
