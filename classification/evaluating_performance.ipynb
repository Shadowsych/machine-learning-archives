{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Classification Models Performance\n",
    "There are multiple methods to evaluate classification model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# False Positive and Negatives\n",
    "We cannot always assume that the classification is correct, even for our training data set.\n",
    "\n",
    "False Positive (Type 1 Error): Model predicted 0 (negative) but actual was 1 (positive).  \n",
    "False Negative (Type 2 Error): Model predicted 1 (positive) but actual was 0.  \n",
    "True Positive: Model predicted 1 (positive) and actual was 1 (positive).  \n",
    "True Negative: Model predicted 0 (negative) and actual was 0 (negative).\n",
    "\n",
    "\n",
    "### Visual Example\n",
    "<img src=\"images/evaluation/false_positives_negatives.png\" height=\"75%\" width=\"75%\"></img>\n",
    "- The \"red\" data points are the testing data points, classified as either 0 or 1\n",
    "- The \"blue\" data points are the predicted classifications of the red (testing) data points\n",
    "- The \"gray\" data points are the classification of the blue (predicted) data points\n",
    "\n",
    "As we can see, some of the predicted values were false for the testing set.\n",
    "- Type 1 (False Positive) Error with data point #3\n",
    "- Type 2 (False Negative) Error with data point #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix\n",
    "<img src=\"images/evaluation/confusion_matrix.png\" height=\"75%\" width=\"75%\"></img>\n",
    "\n",
    "We can model the \"False Positive and Negatives\" intuition using a Confusion Matrix.\n",
    "- 40 data points were actually 0\n",
    "    - 35 data points predicted 0\n",
    "    - 5 data points predicted 1\n",
    "- 60 data points points were actually 1\n",
    "    - 10 data points predicted 0\n",
    "    - 50 data points predicted 1\n",
    "    \n",
    "Therefore, there were 5 + 10 = 15 incorrect predictions and 35 + 50 = 85 correct predictions.\n",
    "- 85% accuracy rate\n",
    "- 15% error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision vs Recall vs Accuracy\n",
    "<img src=\"images/evaluation/precision_recall_accuracy.png\" height=\"75%\" width=\"75%\"></img>\n",
    "- https://towardsdatascience.com/precision-vs-recall-386cf9f89488\n",
    "\n",
    "Precision refers to the percentage of your results that are relevant.  \n",
    "Recall refers to the percentage of the total relevant results correctly classified by the model.  \n",
    "Accuracy refers to the percentage of correct results correctly classified by the model.\n",
    "\n",
    "### Example of Differences\n",
    "Let's say a model classifies cats and dogs in a photograph.\n",
    "\n",
    "The model identifies 18 cats and 18 dogs in a photograph containing 12 dogs and 24 cats.\n",
    "- Of the 18 identified cats, 12 are actually cats (true positive) and 6 are dogs (false positive)\n",
    "- Of the 18 identified dogs, 12 are cats (false negative) and 6 are actually dogs (true negative)\n",
    "\n",
    "The precision of identifying cats is 12 / 18. The recall of identifying cats is 12 / 24.  \n",
    "The precision of identifying dogs is 6 / 18. The recall of identifying dogs is 6 / 12.  \n",
    "The accuracy of identifying cats and dogs is 18 / 36."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Paradox\n",
    "Let's say you have a training set of 100 people, and the model predicts if they have cancer.\n",
    "- In reality, only 2 of them have cancer\n",
    "\n",
    "You create 2 classification models.\n",
    "\n",
    "### Model 1:\n",
    "Your model simply assumes nobody has cancer.\n",
    "\n",
    "You find out 2 people actually had cancer. Therefore, 2 incorrect predictions, so the accuracy of this model is 98%.\n",
    "\n",
    "### Model 2:\n",
    "You run a random forest classification, and you predict 5 people have cancer.\n",
    "\n",
    "You find out 2 people actually had cancer, and 3 people did not have cancer. Therefore, 3 incorrect predictions, so the accuracy of this model is 97%.\n",
    "\n",
    "### Comparision of Models\n",
    "Even though Model 2 is less accurate, it's a far more useful algorithm than just assuming no one has cancer.\n",
    "\n",
    "In conclusion, accuracy is not the greatest judgement of a classification model. We need to delve deeper and find out exactly what the model is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cumulative Accuracy Profile (CAP) Curve\n",
    "We can see how much gain we receive from using a specific model.\n",
    "- Hit Ratio: Return on investment\n",
    "\n",
    "### CAP Curve Example\n",
    "<img src=\"images/evaluation/cap_random_scenario.png\" height=\"50%\" width=\"50%\"></img>\n",
    "- https://medium.com/@lotass/classification-models-performance-evaluation-c3a91562793\n",
    "\n",
    "10% of a random set of people purchased the product.\n",
    "\n",
    "#### But can we improve this? Yes, we can!  \n",
    "1. Inspect your training data, take a group of customers who bought the product, then extract their featuers (independent variables) such as browsing device, age, salary, etc.  \n",
    "2. Fit that group data set to a classification model.  \n",
    "3. Make a prediction of whom from the group data set would purchase the product.  \n",
    "4. Contact the predicted people that would purchase the product.  \n",
    "5. Measure the response of the people that we predicted would purchase the product in a CAP curve.\n",
    "\n",
    "<img src=\"images/evaluation/cap_directed_scenario.png\" height=\"50%\" width=\"50%\"></img>\n",
    "- This CAP curve was determined using a Logistic Regression model\n",
    "\n",
    "As we can see in this CAP curve, we did a lot better than the random scenario.\n",
    "\n",
    "#### Let's use a different classification model!\n",
    "What would the CAP curve look like if we tried a different classiffication model?\n",
    "\n",
    "<img src=\"images/evaluation/svm_lr_cap_curves.png\" height=\"60%\" width=\"60%\"></img>\n",
    "\n",
    "It seems as though the SVM model performs worse than the Logistic Regression model.\n",
    "\n",
    "### CAP Curve Analysis\n",
    "The better your model, the larger will be the area between its CAP curve and the random scenario straight line.\n",
    "\n",
    "Although, it's difficult to measure the area between curves, so we're going to try an easier approach.\n",
    "\n",
    "1. Draw a line on the 50% point of the x-axis.  \n",
    "2. From that intersection point of your model, project it to the y-axis to get an X% value.\n",
    "\n",
    "<img src=\"images/evaluation/cap_curve_analysis.png\" height=\"60%\" width=\"60%\"></img>\n",
    "- If X < 60% (6000) then you have a rubbish model\n",
    "- If 60% < X < 70% 7000) then you have a poor model\n",
    "- If 70% < X < 80% (8000) then you have a good model\n",
    "- If 80% < X < 90% (9000) then you have a very good model\n",
    "- If 90% < X < 100% (10,000) then your model is too good to be true!\n",
    "    - This usually happens due to overfitting to the training group data set\n",
    "        - The model would predict great on a data point from the trained group data set, but it'll predict poorly for an unseen data point"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
